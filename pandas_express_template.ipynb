{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pandas Express \n",
    "###An express guide to becoming a Kung Fu Pandas master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://vignette1.wikia.nocookie.net/kungfupanda/images/8/88/Po2.jpg/revision/latest?cb=20100726062228\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defeating the evil snow leopard Tai Lung, our favorite kung fu panda master Po returns to the Valley of Peace to help his father Mr. Ping with his noodle restaurant. Mr. Ping's noodle restaurant hasn't been doing so well, so Po is determined to help his dad figure out what he can do to improve his restaurant. Luckily, Po has been trained in the revered and ancient Python style of Shaolin martial arts and will analyze a dataset from Yelp to save his father's restaurant, like a true Kung Fu Pandas master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Tools\n",
    "This tutorial will walk you through doing some basic data cleaning and exploratory analysis with Pandas and a suite of other Python data analysis tools. Below are a few of the tools we will be using:\n",
    "\n",
    "* [numpy](http://docs.scipy.org/doc/numpy-dev/user/index.html), for arrays\n",
    "* [pandas](http://pandas.pydata.org/), for data frames\n",
    "* [matplotlib](http://matplotlib.org/), for plotting\n",
    "* [seaborn](http://stanford.edu/~mwaskom/software/seaborn/), for making plots pretty\n",
    "* [statsmodels](http://statsmodels.sourceforge.net/), for statistical analysis\n",
    "* [sklearn](http://scikit-learn.org), for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "import sklearn \n",
    "\n",
    "# iPython command to format matplotlib plots\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble importing any of the packages, you might need to install it first from the website or, if you're on Mac OS or ubuntu, from the console with: `pip install <name of package>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Dataset\n",
    "We will be using a dataset of Yelp reviews provided by the [Yelp Dataset Challenge](http://www.yelp.com/dataset_challenge). The download consists of the following files in JSON format:\n",
    "* business.json - information on businesses\n",
    "* review.json - text and metadata of reviews\n",
    "* tip.json - text and metadata of tips\n",
    "* user.json - information on users\n",
    "* checkin.json - number of checkins at each business\n",
    "\n",
    "In this tutorial, we will be primarily focused on the business.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Loading in and cleaning the data\n",
    "The Yelp dataset is in JSON format, [which you can read about here](https://en.wikipedia.org/wiki/JSON) if you are at all interested. Luckily, Python (and most other programming languages) has packages for parsing and reading JSON file formats. In this case, we are going to use Python's JSON file reader to read in the file and then convert it to a Pandas DataFrame. \n",
    "\n",
    "(Note: Pandas also has a [`read_json`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) function that reads in a JSON file into a DataFrame directly, but in this case, we had to do a little cleanup to remove trailing whitespace at the end of each line, so we didn't use that particular function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "'''\n",
    "load_data(filepath) \n",
    "Given a filepath to a JSON file, loads in the file and formats the JSON\n",
    "'''\n",
    "def load_data(filepath):\n",
    "    data = []\n",
    "    \n",
    "    # Open file and read in line by line\n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            # Strip out trailing whitespace at the end of the line\n",
    "            data.append(json.loads(line.rstrip()))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data('data/business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now let's take a peek inside\n",
    "The [Pandas documentation](http://pandas.pydata.org/pandas-docs/stable/api.html#dataframe) has a full list of functions, but below are some helpful ones for doing some initial poking around. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "head(n=5)\n",
    "Returns first n rows\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "info(verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)\n",
    "Concise summary of a DataFrame.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "describe(percentile_width=None, percentiles=None, include=None, exclude=None)\n",
    "Generate various summary statistics, excluding NaN values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "shape\n",
    "Attribute of a DataFrame as (rows, columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most commonly used operations on Dataframes are various types of indexing, filtering, and slicing. Pandas has a number of different ways to do these operations ([check out this whole page of documentation about it](http://pandas.pydata.org/pandas-docs/stable/indexing.html)), but below are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select a column (returns a Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select multiple columns (returns a DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter a column on a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cleaning and formatting columns\n",
    "\n",
    "To scope out the competition, Po might be interesting in filtering the dataset to see what other Chinese restuarants are out there and see what types of attributes they offer (e.g. if they offer Take-out, etc). Unfortunately, our `attributes` and `categories` columns aren't very user friendly and filter-able, so we might need to do a little bit of cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Format attributes column\n",
    "Taking a closer look at the `attributes` column, we see that all the attributes for a business are still stored in mulitple levels of nested JSON (which is really annoying to deal with in a DataFrame because you can't sort and do various operations with it). Luckily, the `pandas.io.json` library came equipped with a nifty little function called [`json_normalize`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html) which takes a JSON object, \"flattens\" any nested objects, and returns a Pandas DataFrame. So we will use this function to create a `attributes_df` DataFrame to store the attributes of a business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format the attributes as a list of dict objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Pandas attempted to infer the datatypes of our attributes when it loaded in the data with `json_normalize`, most of the columns came in as object types, so one thing we can do is try to convert the columns to numeric values whenever possible using the `convert_objects` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert objects to a numeric datatype if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the attributes that are still non-numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_numeric_attributes = attributes_df.select_dtypes(include=['object']).columns\n",
    "numeric_attributes = attributes_df.select_dtypes(exclude=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attributes_df[non_numeric_attributes].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to get non-numerical/categorical data into a more consummable format is to create dummy/indicator variables for them. To do this, we can use a handy-dandy Pandas function [`get_dummies`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) to help create dummy variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for non-numeric attributes\n",
    "\n",
    "# Drop non-numeric attributes from attributes_df\n",
    "\n",
    "# Add the dummy variables to attributes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to get more picky, there's probably more work we can do with cleaning the attributes, but it's in a good spot now for us to save it and merge it back with our original `business_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the list of attributes for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge it with our original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop our original attributes column that is no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create dummy/indicator variables for categories column\n",
    "\n",
    "Next up, if we now look at the `categories` column, we see that the categories are stored as lists. While that's easy to read, it's not actually in the most usable format if we're going to conduct any data analysis (for example, if we wanted to know how many Chinese restaurants we had in our dataset). We want to create dummy variables for the categories similar to what we did for attributes, but the categories pose an interesting dilemma because they are stored as lists. So we are going to use a slightly modified version of [`get_dummies`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) by splitting the lists up using a spring operator and then creating the dummy variables from a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the list of categories for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge it with our original dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of dropping the `categories` column, we're going to keep it around, but reformat it as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df['categories'] = business_df['categories'].apply(lambda x: tuple(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do things like, say, filter `business_df` for all Chinese restaurants, or do a count of the number of Chinese restaurants to size up the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_df[business_df['Chinese'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df['Chinese'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####So far so good!\n",
    "There is definitely more clean-up work to be done with our datasets (we can continue to work with the `neighbors` or `hour` columns), but for now, we're ready to start doing some analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Descriptive Statistics\n",
    "First, we might be interested in some basic descriptive statistics about our dataset. With a series of filters and statistical functions, we can do some initial exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Looking at relevant attributes\n",
    "If we look at our attributes again, we see that there is a good amount of missing info (because, for example, certain attributes like `Hair Types Specialized In` simply aren't going to be applicable to any businesses other than hair salons). Since we are looking at restaurants for now, we might want to know the attributes that have the most non-null values, and therefore potentially the more important attributes for restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of non-null attributes \n",
    "\n",
    "# Sort the attribute counts\n",
    "\n",
    "# Print the top 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Top restaurant categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of restaurants are in each category\n",
    "\n",
    "# Sort the category counts\n",
    "\n",
    "# Print the top 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the categories that are not relevant to restaurants \n",
    "non_restaurant_categories = restaurant_category_counts[restaurant_category_counts <= 0].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to also generate some plots to visualize our data. Python has a number of visualization libraries, some built on top of others. We will primarily be using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/index.html), which is a library based on [matplotlib](http://matplotlib.org/), but feel free to check out some of the other options as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ratings Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (8, 4)})\n",
    "\n",
    "data = business_df['stars']\n",
    "sns.distplot(data, kde=False, bins=10)\n",
    "\n",
    "# Add headers and labels to the plot\n",
    "plt.title('Ratings Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print some descriptive statistics\n",
    "print \"Mean: %f\" % data.mean()\n",
    "print \"Min: %f\" % data.min()\n",
    "print \"Max: %f\" % data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ratings Distribution for Chinese Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (8, 4)})\n",
    "\n",
    "data = business_df[business_df['Chinese'] == 1]['stars']\n",
    "sns.distplot(data, kde=False, bins=10)\n",
    "\n",
    "# Add headers and labels to the plot\n",
    "plt.title('Ratings Distribution for Chinese Restaurants')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print some descriptive statistics\n",
    "print \"Mean: %f\" % data.mean()\n",
    "print \"Min: %f\" % data.min()\n",
    "print \"Max: %f\" % data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Analysis\n",
    "There are a variety of methods that we could use to conduct the analysis we want to do, but here, we just do a very simple classifier to see what features are important for creating a good restaurant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Select the data we want and format it for use with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get just the numeric columns\n",
    "numeric_only = business_df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter for the attributes and categories we have most information about\n",
    "filtered_df = (numeric_only\n",
    "                .drop('open', axis=1)\n",
    "                .drop(sorted_attributes[20:].index.values, axis=1)\n",
    "                .drop(['latitude', 'longitude'], axis=1)\n",
    "                .drop(non_restaurant_categories, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we just replace any nan values with a 0, but in reality, there are better ways of filling in missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill any na values with 0\n",
    "filtered_df = filtered_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test sets and pull out the labels (in this case we are looking at stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into data and labels\n",
    "data = filtered_df[filtered_df['Restaurants'] == 1].drop('stars', axis=1)\n",
    "labels = filtered_df[filtered_df['Restaurants'] == 1]['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Format labels as dummy variables for classification\n",
    "labels = labels.astype(str).str.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split into test and train sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data.values, labels.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Using a random forest classifier to look at feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = data.columns.values\n",
    "\n",
    "# Instantiate the classifier\n",
    "clf = RandomForestClassifier(n_estimators = 100, max_features='auto', max_depth=4)\n",
    "\n",
    "# Fit the classifier to our training data\n",
    "clf = clf.fit(train_data,train_labels)\n",
    "\n",
    "# look at feature importance\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "sorted_features = []\n",
    "for f in range(len(indices)):\n",
    "    print(\"%s | %f\" % (features[indices[f]], importances[indices[f]]))\n",
    "    sorted_features.append(features[indices[f]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on this analysis, Po might try to focus on the attributes that had large importance in determining a restaurant's rating. There is, however, a lot of additional statistical and machine learning techniques we can use to better help Po conduct his analysis. \n",
    "\n",
    "Stay tuned for future tutorials on how Po can use techniques like natural language processing or network analysis to better help his father's restaurant!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
